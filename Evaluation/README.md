# Evaluation

This folder contains the resources and results of the experiments we run to evaluate OATAPI.

## Experiment 1
This evaluation consisted on a survey asking users to determine whether the API paths built by OATAPI are indistinguishable from those generated by application developers in real use cases.

### Settings:
First, we generated a corpus containing: 1) A set of CQs and its ontologies from different domains, and 2) Pre-existing API paths gathered from real use cases for the CQs and ontologies mentioned in point 1. Then, we run OATAPI to automatically generate a set of API paths. These paths were generated for the CQs and ontologies mentioned in point 1. Our [corpus](https://github.com/oeg-upm/oatapi/blob/main/Evaluation/Indistinguishable/TestbedFromFieldResearchWork.xlsx) and the [script](https://github.com/oeg-upm/oatapi/tree/main/Evaluation/Indistinguishable/indistinguishable-evaluation.py) we used for the automatic generation of API paths are available in the *Indistinguishable* folder.

Some elements of our corpus (ontologies, CQs and pre-existing API paths) were gathered from the [*Ciudades Abiertas*](https://ciudadesabiertas.es) (Open Cities (OC)), a project where several Spanish cities have been working together to create a shared set of ontologies to provide homogeneous data access in their open data portals and APIs. In addition, other elements were obtained from the Zaragoza city council (ZGZ), such elements have been generated as part of the Zaragoza's open-data-by-default policy. One of the main assets of this policy is the Zaragoza's Knowledge Graph, which clusters the open data of this city council (which are described with ontologies) in order to exploit the knowledge of the city [1]. In  both cases an attempt has been made to generate their APIs; however, these APIs are not based on or generated from the CQs or the ontology serialization. Finally, it is worth mentioning that all gathered CQs and API paths were originally written in Spanish. Therefore, we translated these CQs and API paths into English to generate the API paths with OATAPI, and to be able to compare these API paths with the translated API paths from these real uses cases.

Second, we created a questionnaire we used to ask participants about: a) their background, b) their skills on running basic GET operations in REST API calls, c) for each CQ, whether the API paths presented to them had been produced manually or automatically, and d) their API path preference. For each CQ, the questionnaire first presented to the participants the API path manually generated by developers (denoted by "1st"), and then it presents the API path automatically generated by OATAPI (denoted by "2nd"). Participants did not know which paths had been generated by OATAPI. The [questionnaire template](https://github.com/oeg-upm/oatapi/blob/main/Evaluation/Indistinguishable/Questionnaire-Template.pdf) and the [results of the survey](https://github.com/oeg-upm/oatapi/blob/main/Evaluation/Indistinguishable/Questionnaire%20on%20generating%20REST%20APIs%20from%20ontology%20artefacts%20.csv) are available in the *Indistinguishable* folder.

## Experiment 2
This evaluation compares the API paths automatically generated by OATAPI against a baseline built by hand.

### Settings:
First, we generated a corpus which contains: 1) a set of CQs and its ontologies from different domains, 2) API paths manually built, following the steps of our method, for these CQs and ontologies, and 3) API paths built by OATAPI taking these CQs and ontologies as input.

The ontologies and CQs of our corpus were gathered from well-known works from the state of the art like the VideoGame - VGO [2], Public Procurement - PPROC [3], Software - SWO [4], and SAREF for Environment - SAREF4ENVI [5].  We also took some of the ontology artefacts from ontologies of Open Cities (Ciudades Abiertas) and Zaragoza city council projects described in the previous experiment. It is worth mentioning that for manual building of the API paths, we originally analyzed a total of 8 ontology serializations and 228 CQs. However, we only use 50 valid CQs to build the API paths and we included an "undefined" value in those rows containing excluded CQs, further comments on each case are also provided. The excluded CQs were those that are ambiguous, contain elements that are not defined in the ontology (e.g., synonyms), are complex, and contain terms that correspond to properties of the ontology that do not have domain and range. The [file](https://github.com/oeg-upm/oatapi/blob/main/Evaluation/Similarity/CorpusWithAPIs-ManuallyGenerated.xlsx) containing all the CQs initially analyzed is available in the *Similarity* folder.


Second, we use the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) [6] metric to compare the similarity between the API paths from our corpus. Therefore,
for each CQ we compare the API path manually built and the API path built by OATAPI. More precisely, we use the ROUGE-N metric which allows measuring the number of N matching tokens/words. Therefore, we use ROUGE-1 to measure the match-rate of one token between the API paths because each API path is provided as a single token. ROUGE metric allows us to calculate the recall, precision, and F1 score between the API paths. In order to calculate ROUGE-1, we use the [rouge Python library](https://pypi.org/project/rouge). Our [final corpus](https://github.com/oeg-upm/oatapi/blob/main/Evaluation/Similarity/CorpusWithAPIs-AutomaticallyGenerated.xlsx), the [script](https://github.com/oeg-upm/oatapi/tree/main/Evaluation/Similarity/similarity-evaluation.py) we used for the API paths comparison, and the [results](https://github.com/oeg-upm/oatapi/blob/main/Evaluation/Similarity/SimilarityEvaluation-Results.xlsx) are available in the *Similarity* folder. (Note that in our paper [7] we group the values from these results as *True Positives* to represent the sum of the each metric value obtained after executing each comparison).

### References
[1] Espinoza-Arias P, Fernández-Ruiz MJ, Morlán-Plo V, Notivol-Bezares R, Corcho O. The Zaragoza’s Knowledge Graph: Open Data to Harness the City Knowledge. Information. 2020; 11(3):129.

[2] Parkkila, J., Radulovic, F., Garijo, D., Poveda-Villalón, M., Ikonen, J., Porras, J., & Gómez-Pérez, A. (2017). An ontology for videogame interoperability. Multimedia tools and applications, 76(4), 4981-5000.

[3] Muñoz-Soro, J. F., Esteban, G., Corcho, O., & Serón, F. (2016). PPROC, an ontology for transparency in public procurement. Semantic Web, 7(3), 295-309.

[4] Malone, J., Brown, A., Lister, A. L., Ison, J., Hull, D., Parkinson, H., & Stevens, R. (2014). The Software Ontology (SWO): a resource for reproducibility in biomedical data analysis, curation and digital preservation. Journal of biomedical semantics, 5(1), 1-13.

[5] TS, E. (2020). 103 264 V3. 1.1-SmartM2M; Smart Applications; Reference Ontology and oneM2M Mapping. Technical report.

[6] Lin, C. Y. (2004, July). Rouge: A package for automatic evaluation of summaries. In Text summarization branches out (pp. 74-81).

[7] Espinoza-Arias, P., Garijo, D., & Corcho, O. (2022). Extending Ontology Engineering Practices to Facilitate Application Development. *to appear In International Conference on Knowledge Engineering and Knowledge Management*.
